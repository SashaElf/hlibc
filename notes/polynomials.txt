03:12 < graff>  mean do they just sit around trying polynomial series with various fractions until it works?
03:12 < graff> or wtf
03:12 < graff> i know it's related to chebyshev and fourier's work, but I don't have the references of where i read that
03:13 < graff> probably just wikipedia, so I am pretty lost here on this
03:14 < alphamule> Can't even look at them, wifi flaking...  Some of them they actually did find by creating series that way.
03:14 < Cale> graff: I'm not entirely sure. For which domain is that approximation valid?
03:14 < alphamule> But some of them are from geometry.
03:14 < Cale> graff: There are lots of tools which can be used to obtain polynomial approximations to curves
03:15 < alphamule> And once you get a computer... yeah.
03:15 < PlanckWalk> More usually polynomial approximations are found by solving for coefficients that minimize some error function.
03:15 < PlanckWalk> The appropriate choices of error functions vary, and there's also often some adjustments made for speed of 
                    computation.
03:15 < PlanckWalk> I.e. not just degree of polynomia
03:16 < Cale> Yeah, you could look up Taylor series expansions, or Bezier curves.
03:16 -!- horrst [~horst@x59cc8b49.dyn.telefonica.de] has joined ##math
03:16 < PlanckWalk> Taylor series give the best approximation locally to some chosen point, for example.
03:16 < graff> Cale: they say [0,pi/2], but I bet it's actually better with [0, 1] emitted from the remez algorithm
03:17 < graff> PlanckWalk: so that is what I thought, these can probably be derived by running some other series
03:17 < PlanckWalk> Truncated Taylor series, rather.
03:17 -!- a____ptr [uid141280@gateway/web/irccloud.com/x-drkcytomvsjevrhf] has joined ##math
03:17 < alphamule> You want to know pi/4?  Just integrate for the quarter circle using sqrt(1-x^2), for example.
03:18 < PlanckWalk> For more distributed error behaviour, you can do things like minimize RMS error or absolute error or something.
03:18 < alphamule> Then you find some series that leads to that approximation.
03:18 < graff> akrv: i think that what you are describing must be the internals of how the remez algorithm works
03:18 < graff> alphamule: ^
03:19 < PlanckWalk> Those give you some (sometime piecewise) integrals to evaluate for error in terms of the coefficients, and you can 
                    use usual calculus methods to minimize that.
03:20 -!- smerdyakow [~OOO133777@n218250224140.netvigator.com] has quit [Ping timeout: 276 seconds]
03:21 < PlanckWalk> Or alternatively, choose an error bound and find a polynomial that meets those bounds while being fast to evaluate.
03:22 < PlanckWalk> That latter approach usually depends much more upon the specifics of the hardware, though.
03:23 -!- user1 [~user@85.175.31.57] has quit [Quit: WeeChat 2.1]
03:23 < Cale> I remember when I was working for a researcher at McMaster university one summer, on a pipeline scheduler for PPC + 
              Altivec that was part of a special purpose compiler for signal processing applications he was working on, there was a 
              piece of vectorised code for computing sines and cosines we were using as a test case, and it used a polynomial spline 
              approximation to 1/16 of the unit circle, and then various symmetries
03:23 < Cale> of the circle to obtain the rest
03:24 -!- user1 [~user@85.175.31.57] has joined ##math
03:24 < alphamule> Well, yeah, most people don't go beyond 1/16th before using a huge table or the like.
03:24 < Cale> hm?
03:24 < alphamule> It's interesting how some of those old multiplication approximations can be used in reverse.
03:25 < alphamule> Like, you have a fast multiply to get a good approximation.
03:25 < Cale> I mean that it used a polynomial spline that went from (0,0) to (cos(pi/8), sin(pi/8)) along the unit circle
03:25 -!- zergut [uid221522@taskhive/translator/zergut] has joined ##math
03:25 -!- user3 [~user@85.175.31.57] has joined ##math
03:26 < Cale> and then using various clever reflections and identities about the trig functions, managed to treat the rest of the circle
03:26 < alphamule> The sine/cosine table naturally repeats... yes
03:27 -!- gareppa [~gareppa@unaffiliated/gareppa] has joined ##math
03:28 < Cale> Not just that it repeats every 2pi due to periodicity, but also once you have the sine and cosine of just a small 
              fraction of the unit circle, you can get all the other values with simple arithmetic
03:28 < alphamule> Even better:  sine table is a cosine table
03:28 -!- Tannishpage [~Tannishpa@101.165.134.14] has quit [Quit: beep beep bop]
03:29 -!- Tannishpage [~Tannishpa@101.165.134.14] has joined ##math
03:29 < alphamule> And cosine is of course an even function
03:29 < alphamule> :)
03:30 -!- econoraptorman [uid147250@gateway/web/irccloud.com/x-dswtlcnzjmaywjlg] has quit [Quit: Connection closed for inactivity]
03:30 < alphamule> Yeah, there comes a point though, where it's not worth doing this anymore.  I'm actually surprised they went to 
                   1/16th rotation
03:30 < alphamule> Yeah, there comes a point though, where it's not worth doing this anymore.  I'm actually surprised they went to 
                   1/16th rotation
03:31 < alphamule> Then again, my experience is different.
03:31 < Cale> Well, the smaller a fragment you can get, the closer to a straight line that is, and so you require fewer terms to get a 
              decent approximation
03:32 -!- tomboy64 [~tomboy64@gateway/tor-sasl/tomboy64] has quit [Quit: Off to see the wizard.]
03:34 < Cale> I remember the loop for computing sines and cosines would compute 4 sines and 4 cosines each iteration (using vector 
              instructions), and it was somewhere around 30 instructions that we got to run in just 16 clock cycles due to carefully 
              issuing them to the processor in a particular order such that its various components would be able to handle them in 
              parallel (my program did the search for this ordering by simulating
03:34 < Cale> the behaviour of the CPU)
03:34 < alphamule> I wonder how many actually use that trick with trigonometry to approximate multiplication, in reverse...
03:34 < Cale> Our practical performance, after accounting for loads and stores, was around 2.5 clock cycles per float
03:34 < alphamule> Damn
03:34 < Cale> by comparison, if you call GNU libm's cosine function, it'll take around 200 clock cycles
03:34 < alphamule> Not bad
03:35 < alphamule> Naive implementation?  :D
03:36 < Cale> Its implementation is reasonable, but not vectorised, and it's also a more exact approximation (ours was still very good 
              though -- I can't remember exactly, but it was some small number of ULPs off at worst)
03:37 < alphamule> A lot of the time they're also more portable
03:37 < alphamule> Yeah, not optimized for your system's special features
03:37 < alphamule> Like, at all
03:38 -!- Kingsquee [~kingsquee@d108-180-233-73.bchsia.telus.net] has quit [Quit: Just Monica.]
03:38 < Cale> Right, libm's code just is what it is. The C compiler does what it can, but probably didn't really include all that much 
              in the way of processor-specific tricks.
03:38 < alphamule> They're actually some major work that goes into trying to make code know the most efficient (of available 
                   implementations) of a given function.
03:39 < alphamule> Like, you can't just magically tell the compiler to try them all and pick best, when you're going to be using it on 
                   many machines.  You can't even hand-optimize it with #defines, either.
03:39 -!- kamayuk [810dbbfd@gateway/web/freenode/ip.129.13.187.253] has joined ##math
03:40 < kamayuk> What is the relationship between lattices in SL(2,R) and PSL(2,R) = SL(2,R)/{+-1} ? Is there a 1-1 correspondence?
03:41 < a____ptr> What do you mean by lattice?
03:41 -!- NoMoarSpaceFi [~Forgot_my@65.126.126.193] has quit [Read error: Connection reset by peer]
03:41 < kamayuk> discrete subgroup with finite covolume
03:41 < Cale> This project was sort of coming at the process of compilation from the direction that we didn't mind potentially letting 
              the optimiser run for a week if it resulted in better code, and we assumed we knew which CPU it would run on.


